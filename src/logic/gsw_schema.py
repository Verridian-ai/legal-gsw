"""
GSW Schema - Global Semantic Workspace Data Models

This module defines the core data structures for the Legal GSW system,
implementing the actor-centric episodic memory model from the research.

Based on: arXiv:2511.07587 - Functional Structure of Episodic Memory
"""

from pydantic import BaseModel, Field, PrivateAttr
from typing import List, Optional, Dict, Any
from enum import Enum
from uuid import uuid4
from datetime import datetime


# ============================================================================
# ENUMS - Type Classifications
# ============================================================================

class ActorType(str, Enum):
    """Types of actors that can be extracted from legal documents."""
    PERSON = "person"                    # Parties, judges, children, witnesses
    ORGANIZATION = "organization"        # Courts, departments, employers
    ASSET = "asset"                      # Property, super, vehicles, accounts
    TEMPORAL = "temporal"                # Dates, time periods
    LOCATION = "location"                # Addresses, court locations
    LEGAL_DOCUMENT = "legal_document"    # Orders, applications, affidavits
    ABSTRACT = "abstract"                # Concepts, legal principles


class QuestionType(str, Enum):
    """Types of predictive questions generated by the operator."""
    WHO = "who"
    WHAT = "what"
    WHEN = "when"
    WHERE = "where"
    WHY = "why"
    HOW = "how"
    HOW_MUCH = "how_much"


class LinkType(str, Enum):
    """Types of spatio-temporal links."""
    SPATIAL = "spatial"
    TEMPORAL = "temporal"


# ============================================================================
# CORE GSW ENTITIES
# ============================================================================

class State(BaseModel):
    """
    A state is a condition or description that characterizes how an actor
    exists at a specific point in time.

    Examples:
    - RelationshipStatus: "Married" -> "Separated" -> "Divorced"
    - CustodyArrangement: "Shared care" -> "Primary care with mother"
    - AssetValue: "Valued at $500,000"
    - EmploymentStatus: "Employed as accountant"
    """
    id: str = Field(default_factory=lambda: f"state_{uuid4().hex[:8]}")
    entity_id: str                       # Which actor this state applies to
    name: str                            # State type (e.g., "RelationshipStatus")
    value: str                           # State value (e.g., "Married")
    start_date: Optional[str] = None     # When state began (ISO format or fuzzy)
    end_date: Optional[str] = None       # When state ended (None if current)
    source_chunk_id: str = ""            # Which chunk this was extracted from
    confidence: float = 1.0              # Extraction confidence
    metadata: Dict[str, Any] = Field(default_factory=dict)


class VerbPhrase(BaseModel):
    """
    A verb phrase captures an action or event involving actors.
    Verbs become the LINKS between actors in the GSW model.

    Examples:
    - "filed" (agent: Applicant, patient: Application)
    - "ordered" (agent: Judge, patient: Respondent)
    - "separated" (agent: Husband, patient: Wife)
    """
    id: str = Field(default_factory=lambda: f"verb_{uuid4().hex[:8]}")
    verb: str                            # The action word (e.g., "filed", "ordered")
    agent_id: Optional[str] = None       # Who performed the action
    patient_ids: List[str] = Field(default_factory=list)  # Who/what was affected
    temporal_id: Optional[str] = None    # When (link to temporal entity)
    spatial_id: Optional[str] = None     # Where (link to location entity)
    is_implicit: bool = False            # Was this inferred rather than explicit?
    source_chunk_id: str = ""
    metadata: Dict[str, Any] = Field(default_factory=dict)


class PredictiveQuestion(BaseModel):
    """
    A predictive question represents something that could be asked about
    the situation. The GSW tracks which questions can be answered.

    This implements the "predictive function" of episodic memory -
    the ability to anticipate what information might be needed.

    Examples:
    - "When did the parties separate?" (answerable: Yes, answer: "March 2020")
    - "What is the value of the matrimonial home?" (answerable: No)
    - "Who has primary care of the children?" (answerable: Yes, answer: "Mother")
    """
    id: str = Field(default_factory=lambda: f"q_{uuid4().hex[:8]}")
    question_text: str
    question_type: QuestionType
    target_entity_id: Optional[str] = None  # Which entity this question is about
    answerable: bool = False
    answer_text: Optional[str] = None
    answer_entity_id: Optional[str] = None  # Entity that IS the answer
    source_chunk_id: str = ""              # Where question was generated
    answered_in_chunk_id: Optional[str] = None  # Where answer was found
    metadata: Dict[str, Any] = Field(default_factory=dict)


class SpatioTemporalLink(BaseModel):
    """
    Links entities that share the same spatial or temporal context.

    This is crucial for the GSW model - it binds the "what", "who",
    "when", and "where" together, just as the hippocampus does.

    Examples:
    - Temporal: ["husband", "wife", "marriage_cert"] all linked to "March 2010"
    - Spatial: ["hearing", "judge", "parties"] all linked to "Sydney Family Court"
    """
    id: str = Field(default_factory=lambda: f"link_{uuid4().hex[:8]}")
    linked_entity_ids: List[str]
    tag_type: LinkType
    tag_value: Optional[str] = None      # The actual date/location if known
    source_chunk_id: str = ""
    metadata: Dict[str, Any] = Field(default_factory=dict)


class Actor(BaseModel):
    """
    An actor is the central organizing unit in the GSW model.

    Unlike verb-centric NLP, the GSW organizes information around ACTORS.
    Each actor has:
    - Identity (name, type)
    - Roles (how they function in the situation)
    - States (conditions over time)
    - Links to other entities via spatio-temporal context

    This mirrors how human episodic memory works - we remember WHO was
    involved and WHAT happened to them.
    """
    id: str = Field(default_factory=lambda: f"actor_{uuid4().hex[:8]}")
    name: str
    actor_type: ActorType
    aliases: List[str] = Field(default_factory=list)  # Other names/references
    roles: List[str] = Field(default_factory=list)    # Flexible role strings
    states: List[State] = Field(default_factory=list)
    spatio_temporal_link_ids: List[str] = Field(default_factory=list)
    involved_cases: List[str] = Field(default_factory=list)  # Cross-case tracking
    source_chunk_ids: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)

    def add_state(self, state: State) -> None:
        """Add a new state to this actor."""
        state.entity_id = self.id
        self.states.append(state)

    def get_current_state(self, state_name: str) -> Optional[State]:
        """Get the most recent state of a given type."""
        matching = [s for s in self.states if s.name == state_name and s.end_date is None]
        return matching[-1] if matching else None


# ============================================================================
# CHUNK-LEVEL EXTRACTION OUTPUT
# ============================================================================

class ChunkExtraction(BaseModel):
    """
    The output of the Legal Operator for a single chunk of text.

    This represents one "episode" being added to the workspace.
    The Reconciler will merge this into the Global Workspace.
    """
    chunk_id: str
    source_document_id: str = ""
    situation: str = ""                  # Brief description of what's happening
    background_context: str = ""         # Context from surrounding text

    # Extracted elements (the 6 tasks of the Operator)
    actors: List[Actor] = Field(default_factory=list)
    verb_phrases: List[VerbPhrase] = Field(default_factory=list)
    questions: List[PredictiveQuestion] = Field(default_factory=list)
    spatio_temporal_links: List[SpatioTemporalLink] = Field(default_factory=list)

    # Extraction metadata
    extracted_at: str = Field(default_factory=lambda: datetime.now().isoformat())
    model_used: str = ""
    raw_llm_response: Optional[str] = None

    def get_actor_by_name(self, name: str) -> Optional[Actor]:
        """Find actor by name (case-insensitive)."""
        name_lower = name.lower()
        for actor in self.actors:
            if actor.name.lower() == name_lower:
                return actor
            if name_lower in [a.lower() for a in actor.aliases]:
                return actor
        return None


# ============================================================================
# GLOBAL SEMANTIC WORKSPACE
# ============================================================================

class GlobalWorkspace(BaseModel):
    """
    The Global Semantic Workspace - the central "memory" of the system.

    This is analogous to the hippocampal complex in the brain.
    It binds together all extracted information into a coherent
    representation that can be queried and updated.

    Key operations:
    - Retrieval: Find relevant stored memories (pattern completion)
    - Update: Integrate new information from chunks
    - Query: Answer questions using stored knowledge
    """
    # Core storage (id -> entity)
    actors: Dict[str, Actor] = Field(default_factory=dict)
    states: Dict[str, State] = Field(default_factory=dict)
    verb_phrases: Dict[str, VerbPhrase] = Field(default_factory=dict)
    questions: Dict[str, PredictiveQuestion] = Field(default_factory=dict)
    spatio_temporal_links: Dict[str, SpatioTemporalLink] = Field(default_factory=dict)

    # Entity summaries (generated by Legal Summary)
    entity_summaries: Dict[str, str] = Field(default_factory=dict)

    # Workspace metadata
    created_at: str = Field(default_factory=lambda: datetime.now().isoformat())
    last_updated: str = Field(default_factory=lambda: datetime.now().isoformat())
    chunk_count: int = 0
    document_count: int = 0
    domain: str = ""  # e.g., "family", "criminal"

    # Index structures for fast lookup (private, not serialized)
    _name_to_actor_id: Dict[str, str] = PrivateAttr(default_factory=dict)

    def model_post_init(self, __context) -> None:
        """Rebuild index after loading from JSON."""
        self._name_to_actor_id = {}
        for actor_id, actor in self.actors.items():
            self._name_to_actor_id[actor.name.lower()] = actor_id
            for alias in actor.aliases:
                self._name_to_actor_id[alias.lower()] = actor_id

    def add_actor(self, actor: Actor) -> str:
        """Add or merge an actor into the workspace."""
        self.actors[actor.id] = actor
        self._name_to_actor_id[actor.name.lower()] = actor.id
        for alias in actor.aliases:
            self._name_to_actor_id[alias.lower()] = actor.id
        return actor.id

    def add_verb_phrase(self, verb: VerbPhrase) -> str:
        """Add a verb phrase to the workspace."""
        self.verb_phrases[verb.id] = verb
        return verb.id

    def add_question(self, question: PredictiveQuestion) -> str:
        """Add a predictive question to the workspace."""
        self.questions[question.id] = question
        return question.id

    def add_spatio_temporal_link(self, link: SpatioTemporalLink) -> str:
        """Add a spatio-temporal link to the workspace."""
        self.spatio_temporal_links[link.id] = link
        return link.id

    def add_state(self, state: State) -> str:
        """Add a state to the workspace."""
        self.states[state.id] = state
        return state.id

    def find_actor_by_name(self, name: str) -> Optional[Actor]:
        """Find actor by name or alias."""
        actor_id = self._name_to_actor_id.get(name.lower())
        return self.actors.get(actor_id) if actor_id else None

    def get_unanswered_questions(self) -> List[PredictiveQuestion]:
        """Get all questions that haven't been answered yet."""
        return [q for q in self.questions.values() if not q.answerable]

    def get_answered_questions(self) -> List[PredictiveQuestion]:
        """Get all questions that have been answered."""
        return [q for q in self.questions.values() if q.answerable]

    def get_entities_at_time(self, temporal_value: str) -> List[Actor]:
        """Get all actors linked to a specific time."""
        actor_ids = set()
        for link in self.spatio_temporal_links.values():
            if link.tag_type == LinkType.TEMPORAL and link.tag_value == temporal_value:
                actor_ids.update(link.linked_entity_ids)
        return [self.actors[aid] for aid in actor_ids if aid in self.actors]

    def get_entities_at_location(self, spatial_value: str) -> List[Actor]:
        """Get all actors linked to a specific location."""
        actor_ids = set()
        for link in self.spatio_temporal_links.values():
            if link.tag_type == LinkType.SPATIAL and link.tag_value == spatial_value:
                actor_ids.update(link.linked_entity_ids)
        return [self.actors[aid] for aid in actor_ids if aid in self.actors]

    def get_statistics(self) -> Dict[str, Any]:
        """Get workspace statistics for reporting."""
        return {
            "total_actors": len(self.actors),
            "total_states": len(self.states),
            "total_verb_phrases": len(self.verb_phrases),
            "total_questions": len(self.questions),
            "answered_questions": len(self.get_answered_questions()),
            "unanswered_questions": len(self.get_unanswered_questions()),
            "total_spatio_temporal_links": len(self.spatio_temporal_links),
            "total_summaries": len(self.entity_summaries),
            "chunks_processed": self.chunk_count,
            "documents_processed": self.document_count,
            "domain": self.domain,
            "last_updated": self.last_updated
        }

    def touch(self) -> None:
        """Update the last_updated timestamp."""
        self.last_updated = datetime.now().isoformat()

    def to_toon(self) -> str:
        """
        Export workspace to TOON format (~70% token reduction vs JSON).

        TOON (Token-Oriented Object Notation) compresses context for LLM prompts.
        Use this when injecting workspace context into prompts.
        """
        from src.utils.toon import ToonEncoder
        return ToonEncoder.encode_workspace(self.model_dump())

    def to_toon_summary(self, max_actors: int = 50) -> str:
        """
        Export condensed workspace summary in TOON format.

        Prioritizes most-connected actors and unanswered questions.
        Use for large workspaces where full export exceeds context limits.
        """
        from src.utils.toon import ToonEncoder
        return ToonEncoder.encode_context_summary(self.model_dump(), max_actors)


# ============================================================================
# LEGAL-SPECIFIC EXTENSIONS
# ============================================================================

class LegalCase(BaseModel):
    """
    A legal case containing all extracted information.
    This extends the GSW model for legal domain specifics.
    """
    case_id: str = Field(default_factory=lambda: f"case_{uuid4().hex[:8]}")
    citation: str = ""
    title: str = ""
    jurisdiction: str = ""
    court: str = ""
    date: Optional[str] = None
    case_type: str = ""  # e.g., "parenting", "property", "divorce"

    # GSW extraction for this case
    workspace: GlobalWorkspace = Field(default_factory=GlobalWorkspace)

    # Source tracking
    source_document_id: str = ""
    raw_text_length: int = 0
    chunks_processed: int = 0

    # Extraction metadata
    extracted_at: str = Field(default_factory=lambda: datetime.now().isoformat())
    extraction_model: str = ""
    extraction_errors: List[str] = Field(default_factory=list)


# ============================================================================
# ONTOLOGY CONTEXT (for self-improving feedback loop)
# ============================================================================

class OntologyContext(BaseModel):
    """
    The current state of the ontology - used to feed back into
    the Operator prompt for self-improving extraction.

    This implements Phase 4.5 from the research: the cybernetic
    feedback loop that standardizes vocabulary over time.
    """
    # Frequency counts of extracted values
    actor_types: Dict[str, int] = Field(default_factory=dict)
    role_types: Dict[str, int] = Field(default_factory=dict)
    state_names: Dict[str, int] = Field(default_factory=dict)
    verb_types: Dict[str, int] = Field(default_factory=dict)

    # Standard vocabulary (seed + learned)
    standard_roles: List[str] = Field(default_factory=list)
    standard_states: List[str] = Field(default_factory=list)
    standard_verbs: List[str] = Field(default_factory=list)

    def get_top_n(self, category: str, n: int = 20) -> List[str]:
        """Get top N items from a category by frequency."""
        counts = getattr(self, category, {})
        sorted_items = sorted(counts.items(), key=lambda x: x[1], reverse=True)
        return [item for item, count in sorted_items[:n]]

    def to_prompt_context(self) -> str:
        """Convert to a string for injection into operator prompt."""
        lines = [
            "## Known Vocabulary (from previous extractions)",
            "",
            "### Common Roles:",
            ", ".join(self.get_top_n("role_types", 15)),
            "",
            "### Common States:",
            ", ".join(self.get_top_n("state_names", 15)),
            "",
            "### Common Actions:",
            ", ".join(self.get_top_n("verb_types", 15)),
        ]
        return "\n".join(lines)

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{gpt4,
  title={GPT-4 Technical Report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{transformercircuits,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@misc{instructiontuning,
      title={Finetuned Language Models Are Zero-Shot Learners}, 
      author={Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
      year={2022},
      eprint={2109.01652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kojima2023large,
      title={Large Language Models are Zero-Shot Reasoners}, 
      author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
      year={2023},
      eprint={2205.11916},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{prefixtuning,
      title={Prefix-Tuning: Optimizing Continuous Prompts for Generation}, 
      author={Xiang Lisa Li and Percy Liang},
      year={2021},
      eprint={2101.00190},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{llamaadapter,
      title={LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention}, 
      author={Renrui Zhang and Jiaming Han and Chris Liu and Peng Gao and Aojun Zhou and Xiangfei Hu and Shilin Yan and Pan Lu and Hongsheng Li and Yu Qiao},
      year={2023},
      eprint={2303.16199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{propbank,
    title = "From {T}ree{B}ank to {P}rop{B}ank",
    author = "Kingsbury, Paul  and
      Palmer, Martha",
    editor = "Gonz{\'a}lez Rodr{\'\i}guez, Manuel  and
      Suarez Araujo, Carmen Paz",
    booktitle = "Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02)",
    month = may,
    year = "2002",
    address = "Las Palmas, Canary Islands - Spain",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2002/pdf/283.pdf",
}

@inproceedings{framenet,
  title={The berkeley framenet project},
  author={Baker, Collin F and Fillmore, Charles J and Lowe, John B},
  booktitle={COLING 1998 Volume 1: The 17th International Conference on Computational Linguistics},
  year={1998}
}

@book{levin,
  title={English verb classes and alternations: A preliminary investigation},
  author={Levin, Beth},
  year={1993},
  publisher={University of Chicago press}
}


@article{dpt,
  title={Dependency parsing},
  author={Nivre, Joakim},
  journal={Language and Linguistics Compass},
  volume={4},
  number={3},
  pages={138--152},
  year={2010},
  publisher={Wiley Online Library}
}


@book{verbnet,
  title={VerbNet: A broad-coverage, comprehensive verb lexicon},
  author={Schuler, Karin Kipper},
  year={2005},
  publisher={University of Pennsylvania}
}

@inproceedings{rml,
  title={Dependency-based semantic role labeling of PropBank},
  author={Johansson, Richard and Nugues, Pierre},
  booktitle={Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  pages={69--78},
  year={2008}
}

@article{srlbert,
 author = {Peng Shi and Jimmy Lin},
 journal = {ArXiv},
 title = {Simple BERT Models for Relation Extraction and Semantic Role Labeling},
 volume = {abs/1904.05255},
 year = {2019}
}

@inproceedings{semlink,
  title={Semlink: Linking propbank, verbnet and framenet},
  author={Palmer, Martha},
  booktitle={Proceedings of the generative lexicon conference},
  pages={9--15},
  year={2009},
  organization={GenLex-09, Pisa, Italy}
}


@inproceedings{eventreview,
  title={Event extraction as machine reading comprehension},
  author={Liu, Jian and Chen, Yubo and Liu, Kang and Bi, Wei and Liu, Xiaojiang},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)},
  pages={1641--1651},
  year={2020}
}

@article{xiang2019survey,
  title={A survey of event extraction from text},
  author={Xiang, Wei and Wang, Bang},
  journal={IEEE Access},
  volume={7},
  pages={173111--173137},
  year={2019},
  publisher={IEEE}
}

@inproceedings{ace,
  title={The automatic content extraction (ace) program-tasks, data, and evaluation.},
  author={Doddington, George R and Mitchell, Alexis and Przybocki, Mark A and Ramshaw, Lance A and Strassel, Stephanie M and Weischedel, Ralph M},
  booktitle={Lrec},
  volume={2},
  pages={837--840},
  year={2004},
  organization={Lisbon}
}


@inproceedings{ere,
    title = "A Comparison of the Events and Relations Across {ACE}, {ERE}, {TAC}-{KBP}, and {F}rame{N}et Annotation Standards",
    author = "Aguilar, Jacqueline  and
      Beller, Charley  and
      McNamee, Paul  and
      Van Durme, Benjamin  and
      Strassel, Stephanie  and
      Song, Zhiyi  and
      Ellis, Joe",
    editor = "Mitamura, Teruko  and
      Hovy, Eduard  and
      Palmer, Martha",
    booktitle = "Proceedings of the Second Workshop on {EVENTS}: Definition, Detection, Coreference, and Representation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-2907",
    doi = "10.3115/v1/W14-2907",
    pages = "45--53",
}


@inproceedings{dwdoverlay,
    title = "The {DARPA} {W}ikidata Overlay: {W}ikidata as an ontology for natural language processing",
    author = "Spaulding, Elizabeth  and
      Conger, Kathryn  and
      Gershman, Anatole  and
      Uceda-Sosa, Rosario  and
      Brown, Susan Windisch  and
      Pustejovsky, James  and
      Anick, Peter  and
      Palmer, Martha",
    editor = "Bunt, Harry",
    booktitle = "Proceedings of the 19th Joint ACL-ISO Workshop on Interoperable Semantics (ISA-19)",
    month = jun,
    year = "2023",
    address = "Nancy, France",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.isa-1.1",
    pages = "1--10",
    abstract = "With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, \textit{events} and \textit{actions} are often labeled with eventive nouns (e.g., the process of diagnosing a person{'}s illness is labeled {``}diagnosis{''}), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.",
}


@article{wikidata,
author = {Vrande\v{c}i\'{c}, Denny and Kr\"{o}tzsch, Markus},
title = {Wikidata: a free collaborative knowledgebase},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/2629489},
doi = {10.1145/2629489},
abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
journal = {Commun. ACM},
month = {sep},
pages = {78–85},
numpages = {8}
}



@article{glen,
  title={GLEN: General-Purpose Event Detection for Thousands of Types},
  author={Zhan, Qiusi and Li, Sha and Conger, Kathryn and Palmer, Martha and Ji, Heng and Han, Jiawei},
  journal={arXiv preprint arXiv:2303.09093},
  year={2023}
}


@InProceedings{semlink2,
author="Shi, Lei
and Mihalcea, Rada",
editor="Gelbukh, Alexander",
title="Putting Pieces Together: Combining FrameNet, VerbNet and WordNet for Robust Semantic Parsing",
booktitle="Computational Linguistics and Intelligent Text Processing",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="100--111",
abstract="This paper describes our work in integrating three different lexical resources: FrameNet, VerbNet, and WordNet, into a unified, richer knowledge-base, to the end of enabling more robust semantic parsing. The construction of each of these lexical resources has required many years of laborious human effort, and they all have their strengths and shortcomings. By linking them together, we build an improved resource in which (1) the coverage of FrameNet is extended, (2) the VerbNet lexicon is augmented with frame semantics, and (3) selectional restrictions are implemented using WordNet semantic classes. The synergistic exploitation of various lexical resources is crucial for many complex language processing applications, and we prove it once again effective in building a robust semantic parser.",
isbn="978-3-540-30586-6"
}


@article{eventextract,
  title={Entity, relation, and event extraction with contextualized span representations},
  author={Wadden, David and Wennberg, Ulme and Luan, Yi and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1909.03546},
  year={2019}
}


@article{framenetcompare,
  title={Framenet: Frame semantic annotation in practice},
  author={Baker, Collin F},
  journal={Handbook of Linguistic Annotation},
  pages={771--811},
  year={2017},
  publisher={Springer}
}


@article{attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}

}


@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{fst,
  title={Open-source Frame Semantic Parsing},
  author={Chanin, David},
  journal={arXiv preprint arXiv:2303.12788},
  year={2023}
}

@inproceedings{conceptnet,
  title={Conceptnet 5.5: An open multilingual graph of general knowledge},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}


@inproceedings{verbosity,
author = {von Ahn, Luis and Kedia, Mihir and Blum, Manuel},
title = {Verbosity: a game for collecting common-sense facts},
year = {2006},
isbn = {1595933727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1124772.1124784},
doi = {10.1145/1124772.1124784},
abstract = {We address the problem of collecting a database of ""common-sense facts"" using a computer game. Informally, a common-sense fact is a true statement about the world that is known to most humans: ""milk is white,"" ""touching hot metal hurts,"" etc. Several efforts have been devoted to collecting common-sense knowledge for the purpose of making computer programs more intelligent. Such efforts, however, have not succeeded in amassing enough data because the manual process of entering these facts is tedious. We therefore introduce Verbosity, a novel interactive system in the form of an enjoyable game. People play Verbosity because it is fun, and as a side effect of them playing, we collect accurate common-sense knowledge. Verbosity is an example of a game that not only brings people together for leisure, but also collects useful data for computer science.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {75–78},
numpages = {4},
keywords = {common-sense reasoning, distributed knowledge acquisition, web-based games},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {CHI '06}
}



@article{tegm,
   title={The temporal event graph},
   volume={6},
   ISSN={2051-1329},
   url={http://dx.doi.org/10.1093/comnet/cnx048},
   DOI={10.1093/comnet/cnx048},
   number={4},
   journal={Journal of Complex Networks},
   publisher={Oxford University Press (OUP)},
   author={Mellor, Andrew},
   year={2017},
   month=oct, pages={639–659} }


@inproceedings{future,
    title = "The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction",
    author = "Li, Manling  and
      Li, Sha  and
      Wang, Zhenhailong  and
      Huang, Lifu  and
      Cho, Kyunghyun  and
      Ji, Heng  and
      Han, Jiawei  and
      Voss, Clare",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.422",
    doi = "10.18653/v1/2021.emnlp-main.422",
    pages = "5203--5215",
    abstract = "Event schemas encode knowledge of stereotypical structures of events and their connections. As events unfold, schemas are crucial to act as a scaffolding. Previous work on event schema induction focuses either on atomic events or linear temporal event sequences, ignoring the interplay between events via arguments and argument relations. We introduce a new concept of Temporal Complex Event Schema: a graph-based schema representation that encompasses events, arguments, temporal connections and argument relations. In addition, we propose a Temporal Event Graph Model that predicts event instances following the temporal complex event schema. To build and evaluate such schemas, we release a new schema learning corpus containing 6,399 documents accompanied with event graphs, and we have manually constructed gold-standard schemas. Intrinsic evaluations by schema matching and instance graph perplexity, prove the superior quality of our probabilistic graph schema library compared to linear representations. Extrinsic evaluation on schema-guided future event prediction further demonstrates the predictive power of our event graph model, significantly outperforming human schemas and baselines by more than 17.8{\%} on HITS@1.",
}


@inproceedings{coref,
  title={Improving machine learning approaches to coreference resolution},
  author={Ng, Vincent and Cardie, Claire},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={104--111},
  year={2002}
}

@article{ner,
  title={A survey on deep learning for named entity recognition},
  author={Li, Jing and Sun, Aixin and Han, Jianglei and Li, Chenliang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={34},
  number={1},
  pages={50--70},
  year={2020},
  publisher={IEEE}
}

@book{srl,
  title={Semantic role labeling},
  author={Palmer, Martha and Gildea, Daniel and Xue, Nianwen},
  year={2011},
  publisher={Morgan \& Claypool Publishers}
}


@inproceedings{text2event,
    title = "{T}ext2{E}vent: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
    author = "Lu, Yaojie  and
      Lin, Hongyu  and
      Xu, Jin  and
      Han, Xianpei  and
      Tang, Jialong  and
      Li, Annan  and
      Sun, Le  and
      Liao, Meng  and
      Chen, Shaoyi",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.217",
    doi = "10.18653/v1/2021.acl-long.217",
    pages = "2795--2806",
    abstract = "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
}


@article{gdelt,
  abstract = {GDELT—Global Data on Events, Location and Tone—is a new CAMEO-coded data
set containing more than 200-million geolocated events with global coverage for 1979 to
the present. The data are based on news reports from a variety of international news
sources coded using the Tabari system for events and additional software for location
and tone. The data is freely available and we expect to provide daily updates. This
paper describes the news sources and some of their characteristics, the various pro-
cessing steps that are used in generating the data, some comparisons with the KEDS
Levants/Reuters and ICEWS/Asia data sets, and some visualizations. We conclude
with an outline of planned enhancements to the data in the near future: these include
recoding with new WordNet-enhanced dictionaries, the extension of the CAMEO cod-
ing to incorporate codes for financial events, disease outbreaks and natural disasters,
and the development of an open-source Python-based successor to Tabari which will
use parsed input from existing natural language processing tools.},
  added-at = {2016-01-31T15:17:45.000+0100},
  author = {Leetaru, Kalev and Schrodt, Philip A.},
  biburl = {https://www.bibsonomy.org/bibtex/2429cd41833453ac8e0fc153084570ead/asmelash},
  description = {CiteSeerX — GDELT: Global data on events, location, and tone},
  interhash = {47c6cb987d60b8fac88b0293b38fe01f},
  intrahash = {429cd41833453ac8e0fc153084570ead},
  journal = {ISA Annual Convention},
  keywords = {crisis events gdelt k3 news},
  timestamp = {2016-02-07T13:31:53.000+0100},
  title = {GDELT: Global data on events, location, and tone},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.686.6605},
  year = 2013
}



@article{bs4,
  title={Beautiful soup documentation},
  author={Richardson, Leonard},
  journal={April},
  year={2007}
}


@article{prompt,
  title={Prompt engineering for ChatGPT: A quick guide to techniques, tips, and best practices},
  author={Ekin, Sabit},
  journal={Authorea Preprints},
  year={2023},
  publisher={Authorea}
}


@article{retrieval,
  title={A survey of knowledge-enhanced text generation},
  author={Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  journal={ACM Computing Surveys},
  volume={54},
  number={11s},
  pages={1--38},
  year={2022},
  publisher={ACM New York, NY}
}


@InProceedings{amt,
author="Crowston, Kevin",
editor="Bhattacherjee, Anol
and Fitzgerald, Brian",
title="Amazon Mechanical Turk: A Research Tool for Organizations and Information Systems Scholars",
booktitle="Shaping the Future of ICT Research. Methods and Approaches",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="210--221",
abstract="Amazon Mechanical Turk (AMT), a system for crowdsourcing work, has been used in many academic fields to support research and could be similarly useful for information systems research. This paper briefly describes the functioning of the AMT system and presents a simple typology of research data collected using AMT. For each kind of data, it discusses potential threats to reliability and validity and possible ways to address those threats. The paper concludes with a brief discussion of possible applications of AMT to research on organizations and information systems.",
isbn="978-3-642-35142-6"
}



@article{roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}


@article{snli,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}


@article{gwt,
  title={Global workspace theory of consciousness: toward a cognitive neuroscience of human experience},
  author={Baars, Bernard J},
  journal={Progress in brain research},
  volume={150},
  pages={45--53},
  year={2005},
  publisher={Elsevier}
}


@unpublished{aristotle,
	author = {Aristotle},
	title = {On Interpretation},
	year = {400-300BC}
}

@inproceedings{weaver,
  title={Translation},
  author={Weaver, Warren},
  booktitle={Proceedings of the Conference on Mechanical Translation},
  year={1952}
}


@article{jackendoff,
author = {Jackendoff, Ray},
title = {In Defense of Theory},
journal = {Cognitive Science},
volume = {41},
number = {S2},
pages = {185-212},
keywords = {Mental representation, Syntax, Lexicon, Rules of grammar, Language processing},
doi = {https://doi.org/10.1111/cogs.12324},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12324},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12324},
abstract = {Abstract Formal theories of mental representation have receded from the importance they had in the early days of cognitive science. I argue that such theories are crucial in any mental domain, not just for their own sake, but to guide experimental inquiry, as well as to integrate the domain into the mind as a whole. To illustrate the criteria of adequacy for theories of mental representation, I compare two theoretical approaches to language: classical generative grammar (Chomsky, 1965, 1981, 1995) and the parallel architecture (Jackendoff, 1997, 2002). The grounds for comparison include (a) the internal coherence of the theory across phonology, syntax, and semantics; (b) the relation of language to other mental faculties; (c) the relationship between grammar and lexicon; (d) relevance to theories of language processing; and (e) the possibility of languages with little or no syntax.},
year = {2017}
}


@incollection{truth,
  title={Truth and meaning},
  author={Davidson, Donald},
  booktitle={Philosophy, Language, and Artificial Intelligence: Resources for Processing Natural Language},
  pages={93--111},
  year={1967},
  publisher={Springer}
}


@book{chomsky,
  title={Aspects of the Theory of Syntax},
  author={Chomsky, Noam},
  number={11},
  year={2014},
  publisher={MIT press}
}


@article{socratic,
  title={Elements of the Socratic method: I. Systematic questioning.},
  author={Overholser, James C},
  journal={Psychotherapy: Theory, Research, Practice, Training},
  volume={30},
  number={1},
  pages={67},
  year={1993},
  publisher={Division of Psychotherapy (29), American Psychological Association}
}


@article{squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}


@book{baars,
  title={A cognitive theory of consciousness},
  author={Baars, Bernard J},
  year={1993},
  publisher={Cambridge University Press}
}


@book{situated,
author = {Clancey, William J.},
title = {Situated Cognition: On Human Knowledge and Computer Representations},
year = {1997},
isbn = {0521444004},
publisher = {Cambridge University Press},
address = {USA},
abstract = {From the Publisher:This book is about recent changes in the design of intelligent machines. New computer models of vision and navigation in animals suggest a different way to build machines. Cognition is viewed not just in terms of high-level "expertise," but in terms of the ability to find one's way around the world, to learn new ways of seeing things, and to coordinate activity. This approach is called situated cognition. Situated Cognition differs from other purely philosophical treatises in that Clancey, who has built expert systems for twenty years, explores the limitations of existing computer programs and compares them to human memory and learning capabilities. He examines the implications of situated action from the perspective of artificial intelligence specialists interested in building robots and cognitive scientists seeking to relate descriptive models to neural and social views of knowledge.}
}


@article{crf,
  title={An introduction to conditional random fields},
  author={Sutton, Charles and McCallum, Andrew and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={4},
  number={4},
  pages={267--373},
  year={2012},
  publisher={Now Publishers, Inc.}
}



@Inbook{kripke,
author="Kripke, Saul A.",
title="Naming and Necessity",
bookTitle="Semantics of Natural Language",
year="1972",
publisher="Springer Netherlands",
address="Dordrecht",
pages="253--355",
abstract="I hope that some people see some connection between the two topics in the title. If not, anyway, such connections will be developed in the course of these talks. Furthermore, because of the use of tools involving reference and necessity in analytic philosophy today, our views on these topics really have wide-ranging implications for other problems in philosophy that traditionally might be thought far-removed, like arguments over the mind-body problem or the so-called `identity thesis'. Materialism, in this form, often now gets involved in very intricate ways in questions about what is necessary or contingent in identity of properties --- questions like that. So, it is really very important to philosophers who may want to work in many domains to get clear about these concepts. Maybe I will say something about the mind-body problem in the course of these talks. I want to talk also at some point (I don't know if I can get it in) about substances and natural kinds.",
isbn="978-94-010-2557-7",
doi="10.1007/978-94-010-2557-7_9",
url="https://doi.org/10.1007/978-94-010-2557-7_9"
}

@inproceedings{fillmore,
  title={A frame-semantic approach to semantic annotation},
  author={Lowe, John B},
  booktitle={Tagging Text with Lexical Semantics: Why, What, and How?},
  year={1997}
}

@article{T5,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Liu},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@misc{xlnet,
      title={XLNet: Generalized Autoregressive Pretraining for Language Understanding}, 
      author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
      year={2020},
      eprint={1906.08237},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mplug,
      title={mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections}, 
      author={Chenliang Li and Haiyang Xu and Junfeng Tian and Wei Wang and Ming Yan and Bin Bi and Jiabo Ye and Hehong Chen and Guohai Xu and Zheng Cao and Ji Zhang and Songfang Huang and Fei Huang and Jingren Zhou and Luo Si},
      year={2022},
      eprint={2205.12005},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bertopic,
      title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure}, 
      author={Maarten Grootendorst},
      year={2022},
      eprint={2203.05794},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rocstories,
      title={A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories}, 
      author={Nasrin Mostafazadeh and Nathanael Chambers and Xiaodong He and Devi Parikh and Dhruv Batra and Lucy Vanderwende and Pushmeet Kohli and James Allen},
      year={2016},
      eprint={1604.01696},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{deep_path,
  title={DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning},
  author={Wenhan Xiong and Thi-Lan-Giao Hoang and William Yang Wang},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:20667722}
}

@misc{ties,
      title={TIES-Merging: Resolving Interference When Merging Models}, 
      author={Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal},
      year={2023},
      eprint={2306.01708},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Connectors}},
	url = {https://www.zotero.org/download/connectors},
	urldate = {2025-05-05},
}

@misc{JimenezGutierrez2025HippoRAG,
	title = {From {RAG} to {Memory}: {Non}-{Parametric} {Continual} {Learning} for {Large} {Language} {Models}},
	shorttitle = {From {RAG} to {Memory}},
	url = {http://arxiv.org/abs/2502.14802},
	doi = {10.48550/arXiv.2502.14802},
	abstract = {Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7\% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at https://github.com/ OSU-NLP-Group/HippoRAG.},
	language = {en},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Gutiérrez, Bernal Jiménez and Shu, Yiheng and Qi, Weijian and Zhou, Sizhe and Su, Yu},
	month = feb,
	year = {2025},
	note = {arXiv:2502.14802 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Code and data to be released at: https://github.com/OSU-NLP-Group/HippoRAG},
	file = {PDF:/Users/gabehcoud/Zotero/storage/6MMUNB28/Gutiérrez et al. - 2025 - From RAG to Memory Non-Parametric Continual Learning for Large Language Models.pdf:application/pdf},
}

@misc{gutierrez_hipporag_2025,
	title = {{HippoRAG}: {Neurobiologically} {Inspired} {Long}-{Term} {Memory} for {Large} {Language} {Models}},
	shorttitle = {{HippoRAG}},
	url = {http://arxiv.org/abs/2405.14831},
	doi = {10.48550/arXiv.2405.14831},
	abstract = {In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20\%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.},
	language = {en},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Gutiérrez, Bernal Jiménez and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
	month = jan,
	year = {2025},
	note = {arXiv:2405.14831 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2024. Code and data: https://github.com/OSU-NLP-Group/HippoRAG},
	file = {PDF:/Users/gabehcoud/Zotero/storage/HJJQE996/Gutiérrez et al. - 2025 - HippoRAG Neurobiologically Inspired Long-Term Memory for Large Language Models.pdf:application/pdf},
}

@misc{huet_episodic_2025,
	title = {Episodic {Memories} {Generation} and {Evaluation} {Benchmark} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2501.13121},
	doi = {10.48550/arXiv.2501.13121},
	abstract = {Episodic memory – the ability to recall specific events grounded in time and space – is a cornerstone of human cognition, enabling not only coherent storytelling, but also planning and decision-making. Despite their remarkable capabilities, Large Language Models (LLMs) lack a robust mechanism for episodic memory: we argue that integrating episodic memory capabilities into LLM is essential for advancing AI towards human-like cognition, increasing their potential to reason consistently and ground their output in real-world episodic events, hence avoiding confabulations. To address this challenge, we introduce a comprehensive framework to model and evaluate LLM episodic memory capabilities. Drawing inspiration from cognitive science, we develop a structured approach to represent episodic events, encapsulating temporal and spatial contexts, involved entities, and detailed descriptions. We synthesize a unique episodic memory benchmark, free from contamination, and release open source code and datasets to assess LLM performance across various recall and episodic reasoning tasks. Our evaluation of state-of-the-art models, including GPT-4 and Claude variants, Llama 3.1, and o1-mini, reveals that even the most advanced LLMs struggle with episodic memory tasks, particularly when dealing with multiple related events or complex spatio-temporal relationships – even in contexts as short as 10k-100k tokens.},
	language = {en},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Huet, Alexis and Houidi, Zied Ben and Rossi, Dario},
	month = jan,
	year = {2025},
	note = {arXiv:2501.13121 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/gabehcoud/Zotero/storage/2XDQ75PF/Huet et al. - 2025 - Episodic Memories Generation and Evaluation Benchmark for Large Language Models.pdf:application/pdf},
}

@misc{sarthi_raptor_2024,
	title = {{RAPTOR}: {Recursive} {Abstractive} {Processing} for {Tree}-{Organized} {Retrieval}},
	shorttitle = {{RAPTOR}},
	url = {http://arxiv.org/abs/2401.18059},
	doi = {10.48550/arXiv.2401.18059},
	abstract = {Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20\% in absolute accuracy.},
	language = {en},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Sarthi, Parth and Abdullah, Salman and Tuli, Aditi and Khanna, Shubh and Goldie, Anna and Manning, Christopher D.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.18059 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/gabehcoud/Zotero/storage/MDTA4EXL/Sarthi et al. - 2024 - RAPTOR Recursive Abstractive Processing for Tree-Organized Retrieval.pdf:application/pdf},
}

@misc{das_larimar_2024,
	title = {Larimar: {Large} {Language} {Models} with {Episodic} {Memory} {Control}},
	shorttitle = {Larimar},
	url = {http://arxiv.org/abs/2403.11901},
	doi = {10.48550/arXiv.2403.11901},
	abstract = {Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, braininspired architecture for enhancing LLMs with a distributed episodic memory. Larimar’s memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed—yielding speed-ups of 8-10x depending on the base LLM —as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at https://github.com/IBM/larimar.},
	language = {en},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Das, Payel and Chaudhury, Subhajit and Nelson, Elliot and Melnyk, Igor and Swaminathan, Sarath and Dai, Sihui and Lozano, Aurélie and Kollias, Georgios and Chenthamarakshan, Vijil and Jiří and Navrátil and Dan, Soham and Chen, Pin-Yu},
	month = aug,
	year = {2024},
	note = {arXiv:2403.11901 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: ICML 2024},
	file = {PDF:/Users/gabehcoud/Zotero/storage/M4VMRA8H/Das et al. - 2024 - Larimar Large Language Models with Episodic Memory Control.pdf:application/pdf},
}

@misc{leng_long_2024,
	title = {Long {Context} {RAG} {Performance} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2411.03538},
	doi = {10.48550/arXiv.2411.03538},
	abstract = {Retrieval Augmented Generation (RAG) has emerged as a crucial technique for enhancing the accuracy of Large Language Models (LLMs) by incorporating external information. With the advent of LLMs that support increasingly longer context lengths, there is a growing interest in understanding how these models perform in RAG scenarios. Can these new long context models improve RAG performance? This paper presents a comprehensive study of the impact of increased context length on RAG performance across 20 popular open source and commercial LLMs. We ran RAG workflows while varying the total context length from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three domain-specific datasets, and report key insights on the benefits and limitations of long context in RAG applications. Our findings reveal that while retrieving more documents can improve performance, only a handful of the most recent state of the art LLMs can maintain consistent accuracy at long context above 64k tokens. We also identify distinct failure modes in long context scenarios, suggesting areas for future research.},
	language = {en},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Leng, Quinn and Portes, Jacob and Havens, Sam and Zaharia, Matei and Carbin, Michael},
	month = nov,
	year = {2024},
	note = {arXiv:2411.03538 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 2024 NeurIPS workshop on Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning},
	file = {PDF:/Users/gabehcoud/Zotero/storage/MQZ4SH4I/Leng et al. - 2024 - Long Context RAG Performance of Large Language Models.pdf:application/pdf},
}

@article{eichenbaum_corticalhippocampal_2000,
	title = {A cortical–hippocampal system for declarative memory},
	volume = {1},
	copyright = {2000 Macmillan Magazines Ltd.},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/35036213},
	doi = {10.1038/35036213},
	abstract = {Recent neurobiological studies have begun to reveal the cognitive and neural coding mechanisms that underlie declarative memory — our ability to recollect everyday events and factual knowledge. These studies indicate that the critical circuitry involves bidirectional connections between the neocortex, the parahippocampal region and the hippocampus. Each of these areas makes a unique contribution to memory processing. Widespread high-order neocortical areas provide dedicated processors for perceptual, motor or cognitive information that is influenced by other components of the system. The parahippocampal region mediates convergence of this information and extends the persistence of neocortical memory representations. The hippocampus encodes the sequences of places and events that compose episodic memories, and links them together through their common elements. Here I describe how these mechanisms work together to create and re-create fully networked representations of previous experiences and knowledge about the world.},
	language = {en},
	number = {1},
	urldate = {2025-05-05},
	journal = {Nature Reviews Neuroscience},
	author = {Eichenbaum, Howard},
	month = oct,
	year = {2000},
	note = {Publisher: Nature Publishing Group},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, general, Neurobiology, Neurosciences},
	pages = {41--50},
	file = {Full Text PDF:/Users/gabehcoud/Zotero/storage/DUB9JSV8/Eichenbaum - 2000 - A cortical–hippocampal system for declarative memory.pdf:application/pdf},
}

@incollection{tulving_episodic_1972,
	address = {Oxford, England},
	title = {Episodic and semantic memory},
	abstract = {In this chapter I discuss the possibility that semantic memory, among other things, is not the kind of memory that psychologists have been studying since the time of Ebbinghaus. I will suggest that there are sufficiently fundamental differences between the two forms of memory to recommend that we consider, at least for the time being, the two categories separately. To facilitate subsequent discussion, I will refer to this other kind of memory, the one that semantic memory is not, as 'episodic' memory. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	booktitle = {Organization of memory},
	publisher = {Academic Press},
	author = {Tulving, Endel},
	year = {1972},
	keywords = {Memory, Cognitive Processes, Experiences (Events), Semantics},
	pages = {xiii, 423--xiii, 423},
}

@misc{noauthor_lost_nodate,
	title = {Lost in the middle},
	url = {https://arxiv.org/pdf/2307.03172},
	urldate = {2025-05-05},
	file = {https\://arxiv.org/pdf/2307.03172:/Users/gabehcoud/Zotero/storage/LV55KWJ6/2307.pdf:application/pdf},
}

@misc{liu_lost_2023,
	title = {Lost in the {Middle}: {How} {Language} {Models} {Use} {Long} {Contexts}},
	shorttitle = {Lost in the {Middle}},
	url = {http://arxiv.org/abs/2307.03172},
	doi = {10.48550/arXiv.2307.03172},
	abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	month = nov,
	year = {2023},
	note = {arXiv:2307.03172 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 18 pages, 16 figures. Accepted for publication in Transactions of the Association for Computational Linguistics (TACL), 2023},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/3WWKX7PU/Liu et al. - 2023 - Lost in the Middle How Language Models Use Long Contexts.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/XS9E9N7X/2307.html:text/html},
}



@misc{hsieh_ruler_2024,
	title = {{RULER}: {What}'s the {Real} {Context} {Size} of {Your} {Long}-{Context} {Language} {Models}?},
	shorttitle = {{RULER}},
	url = {http://arxiv.org/abs/2404.06654},
	doi = {10.48550/arXiv.2404.06654},
	abstract = {The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the “needle”) from long distractor texts (the “haystack”), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate 17 long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, almost all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only half of them can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.},
	language = {en},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
	month = aug,
	year = {2024},
	note = {arXiv:2404.06654 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: COLM 2024; Code is available at https://github.com/hsiehjackson/RULER},
	file = {PDF:/Users/gabehcoud/Zotero/storage/BTZKT2T5/Hsieh et al. - 2024 - RULER What's the Real Context Size of Your Long-Context Language Models.pdf:application/pdf},
}

@inproceedings{zhang_inftybench_2024,
	address = {Bangkok, Thailand},
	title = {ı{nftyBench}: {Extending} {Long} {Context} {Evaluation} {Beyond} {100K} {Tokens}},
	shorttitle = {ı{nftyBench}},
	url = {https://aclanthology.org/2024.acl-long.814/},
	doi = {10.18653/v1/2024.acl-long.814},
	abstract = {Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose , the first LLM benchmark featuring an average data length surpassing 100K tokens. comprises synthetic and realistic tasks spanning diverse domains in English and Chinese. The tasks in are designed to require an understanding of long dependencies in contexts and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. Based on , we evaluate several state-of-the-art LLMs tailored for processing long contexts. The experimental results indicate that existing long-context LLMs still require significant advancements to process 100K+ contexts effectively. Furthermore, we present three intriguing analyses regarding the behavior of LLMs processing long context. Our code and data is released.},
	urldate = {2025-05-05},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and Sun, Maosong},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {15262--15277},
	file = {Full Text PDF:/Users/gabehcoud/Zotero/storage/JXEFK3XQ/Zhang et al. - 2024 - ınftyBench Extending Long Context Evaluation Beyond 100K Tokens.pdf:application/pdf},
}

@inproceedings{levy_same_2024,
	address = {Bangkok, Thailand},
	title = {Same {Task}, {More} {Tokens}: the {Impact} of {Input} {Length} on the {Reasoning} {Performance} of {Large} {Language} {Models}},
	shorttitle = {Same {Task}, {More} {Tokens}},
	url = {https://aclanthology.org/2024.acl-long.818},
	doi = {10.18653/v1/2024.acl-long.818},
	abstract = {This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs’ reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs’ on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.},
	language = {en},
	urldate = {2025-05-05},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
	year = {2024},
	pages = {15339--15353},
	file = {PDF:/Users/gabehcoud/Zotero/storage/LKWEHGMZ/Levy et al. - 2024 - Same Task, More Tokens the Impact of Input Length on the Reasoning Performance of Large Language Mo.pdf:application/pdf},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted at NeurIPS 2020},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/QHSCQ85C/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/2I6SQN78/2005.html:text/html},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	month = mar,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Ongoing Work},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/HZ2KR4LG/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/4RYKB6A3/2312.html:text/html},
}

@misc{lee_nv-embed_2025,
	title = {{NV}-{Embed}: {Improved} {Techniques} for {Training} {LLMs} as {Generalist} {Embedding} {Models}},
	shorttitle = {{NV}-{Embed}},
	url = {http://arxiv.org/abs/2405.17428},
	doi = {10.48550/arXiv.2405.17428},
	abstract = {Decoder-only LLM-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce NV-Embed, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last {\textless}EOS{\textgreater} token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position on the MTEB leaderboard (as of May 24 and August 30, 2024, respectively) across 56 tasks, demonstrating the sustained effectiveness of the proposed methods over time. It also achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB. We further provide the analysis of model compression techniques for generalist embedding models.},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
	month = feb,
	year = {2025},
	note = {arXiv:2405.17428 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Information Retrieval},
	annote = {Comment: ICLR 2025 (Spotlight). We open-source the model at: https://huggingface.co/nvidia/NV-Embed-v2},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/V5WYSJTW/Lee et al. - 2025 - NV-Embed Improved Techniques for Training LLMs as Generalist Embedding Models.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/X8UZRK7N/2405.html:text/html},
}

@misc{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {http://arxiv.org/abs/1908.10084},
	doi = {10.48550/arXiv.1908.10084},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = aug,
	year = {2019},
	note = {arXiv:1908.10084 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published at EMNLP 2019},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/NCMD9PQT/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese BERT-Networks.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/BWXF5N74/1908.html:text/html},
}

@misc{muennighoff_generative_2025,
	title = {Generative {Representational} {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2402.09906},
	doi = {10.48550/arXiv.2402.09906},
	abstract = {All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by {\textgreater} 60\% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Su, Hongjin and Wang, Liang and Yang, Nan and Wei, Furu and Yu, Tao and Singh, Amanpreet and Kiela, Douwe},
	month = mar,
	year = {2025},
	note = {arXiv:2402.09906 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 67 pages (16 main), 25 figures, 34 tables},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/3CCQTGPA/Muennighoff et al. - 2025 - Generative Representational Instruction Tuning.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/CVY4ASKA/2402.html:text/html},
}

@article{robertson_probabilistic_2009,
	title = {The {Probabilistic} {Relevance} {Framework}: {BM25} and {Beyond}},
	volume = {3},
	issn = {1554-0669, 1554-0677},
	shorttitle = {The {Probabilistic} {Relevance} {Framework}},
	url = {http://www.nowpublishers.com/article/Details/INR-019},
	doi = {10.1561/1500000019},
	abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970–1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the diﬀerent ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
	language = {en},
	number = {4},
	urldate = {2025-05-05},
	journal = {Foundations and Trends® in Information Retrieval},
	author = {Robertson, Stephen and Zaragoza, Hugo},
	year = {2009},
	pages = {333--389},
	file = {PDF:/Users/gabehcoud/Zotero/storage/Q9XXMJZV/Robertson and Zaragoza - 2009 - The Probabilistic Relevance Framework BM25 and Beyond.pdf:application/pdf},
}

@misc{noauthor_probabilistic_nodate,
	title = {The {Probabilistic} {Relevance} {Framework}: {BM25} and {Beyond}: {Foundations} and {Trends} in {Information} {Retrieval}: {Vol} 3, {No} 4},
	url = {https://dl.acm.org/doi/abs/10.1561/1500000019},
	urldate = {2025-05-05},
}

@misc{ram_-context_2023,
	title = {In-{Context} {Retrieval}-{Augmented} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.00083},
	doi = {10.48550/arXiv.2302.00083},
	abstract = {Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
	month = aug,
	year = {2023},
	note = {arXiv:2302.00083 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: Accepted for publication in Transactions of the Association for Computational Linguistics (TACL). pre-MIT Press publication version},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/D5VJKF2C/Ram et al. - 2023 - In-Context Retrieval-Augmented Language Models.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/PSH6V4HA/2302.html:text/html},
}


@inproceedings{shao_enhancing_2023,
	address = {Singapore},
	title = {Enhancing {Retrieval}-{Augmented} {Large} {Language} {Models} with {Iterative} {Retrieval}-{Generation} {Synergy}},
	url = {https://aclanthology.org/2023.findings-emnlp.620/},
	doi = {10.18653/v1/2023.findings-emnlp.620},
	abstract = {Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to guide retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner: a model`s response to a task input shows what might be needed to finish the task, and thus can serve as an informative context for retrieving more relevant knowledge which in turn helps generate a better response in another iteration. Compared with recent work which interleaves retrieval with generation when completing a single output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.},
	urldate = {2025-05-05},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {9248--9274},
	file = {Full Text PDF:/Users/gabehcoud/Zotero/storage/ZT37XR53/Shao et al. - 2023 - Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy.pdf:application/pdf},
}

@inproceedings{tan_towards_2023,
	address = {Toronto, Canada},
	title = {Towards {Benchmarking} and {Improving} the {Temporal} {Reasoning} {Capability} of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.acl-long.828},
	doi = {10.18653/v1/2023.acl-long.828},
	abstract = {Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TEMPREASON to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach1.},
	language = {en},
	urldate = {2025-05-05},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},
	year = {2023},
	pages = {14820--14835},
	file = {PDF:/Users/gabehcoud/Zotero/storage/VFNEDCI8/Tan et al. - 2023 - Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models.pdf:application/pdf},
}

@misc{fountashuman,
  title={Human-like Episodic Memory for Infinite Context LLMs},
  author={Fountas, Zafeirios and Benfeghoul, Martin and Oomerjee, Adnan and Christopoulou, Fenia and Lampouras, Gerasimos and Ammar, Haitham Bou and Wang, Jun},
  booktitle={The Thirteenth International Conference on Learning Representations}
}


@misc{Edge2025GraphRAG,
	title = {From {Local} to {Global}: {A} {Graph} {RAG} {Approach} to {Query}-{Focused} {Summarization}},
	shorttitle = {From {Local} to {Global}},
	url = {http://arxiv.org/abs/2404.16130},
	doi = {10.48550/arXiv.2404.16130},
	abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as “What are the main themes in the dataset?”, since this is inherently a queryfocused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.},
	language = {en},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Metropolitansky, Dasha and Ness, Robert Osazuwa and Larson, Jonathan},
	month = feb,
	year = {2025},
	note = {arXiv:2404.16130 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {PDF:/Users/gabehcoud/Zotero/storage/TBYQI7DI/Edge et al. - 2025 - From Local to Global A Graph RAG Approach to Query-Focused Summarization.pdf:application/pdf},
}

@misc{chen_walking_2023,
	title = {Walking {Down} the {Memory} {Maze}: {Beyond} {Context} {Limit} through {Interactive} {Reading}},
	shorttitle = {Walking {Down} the {Memory} {Maze}},
	url = {http://arxiv.org/abs/2310.05029},
	doi = {10.48550/arXiv.2310.05029},
	abstract = {Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Chen, Howard and Pasunuru, Ramakanth and Weston, Jason and Celikyilmaz, Asli},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05029 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/RCFQISC3/Chen et al. - 2023 - Walking Down the Memory Maze Beyond Context Limit through Interactive Reading.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/AIGY85Q8/2310.html:text/html},
}

@misc{Guo2024LightRAG,
	title = {{LightRAG}: {Simple} and {Fast} {Retrieval}-{Augmented} {Generation}},
	shorttitle = {{LightRAG}},
	url = {http://arxiv.org/abs/2410.05779},
	doi = {10.48550/arXiv.2410.05779},
	abstract = {Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our LightRAG open-source and available at the link: https://github.com/HKUDS/LightRAG},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao},
	month = apr,
	year = {2025},
	note = {arXiv:2410.05779 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/HN2HB2TL/Guo et al. - 2025 - LightRAG Simple and Fast Retrieval-Augmented Generation.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/8HDMBAVS/2410.html:text/html},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2025-05-06},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2023 Datasets and Benchmarks Track},
	file = {Preprint PDF:/Users/gabehcoud/Zotero/storage/SVA86JJN/Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.pdf:application/pdf;Snapshot:/Users/gabehcoud/Zotero/storage/WXMABHN2/2306.html:text/html},
}

@software{bespoke_curator,
title={Curator: A Tool for Synthetic Data Creation},
  author = {Marten, Ryan* and Vu, Trung* and Ji, Charlie Cheng-Jie and Sharma, Kartik and Pimpalgaonkar, Shreyas and Dimakis, Alex and Sathiamoorthy, Maheswaran},
  month = jan,
  title = {{Curator}},
  year = {2025},
  howpublished = {\url{https://github.com/bespokelabsai/curator}}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550/",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
}

@article{wheeler1997,
  title={Toward a theory of episodic memory: the frontal lobes and autonoetic consciousness.},
  author={Wheeler, Mark A and Stuss, Donald T and Tulving, Endel},
  journal={Psychological bulletin},
  volume={121},
  number={3},
  pages={331},
  year={1997},
  publisher={American Psychological Association}
}

@article{barron2013online,
  title={Online evaluation of novel choices by simultaneous representation of multiple memories},
  author={Barron, Helen C and Dolan, Raymond J and Behrens, Timothy EJ},
  journal={Nature neuroscience},
  volume={16},
  number={10},
  pages={1492--1498},
  year={2013},
  publisher={Nature Publishing Group US New York}
}

@article{thakur2021beir,
  title={Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models},
  author={Thakur, Nandan and Reimers, Nils and R{\"u}ckl{\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2104.08663},
  year={2021}
}

@article{muennighoff2022mteb,
  title={MTEB: Massive text embedding benchmark},
  author={Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  journal={arXiv preprint arXiv:2210.07316},
  year={2022}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{tulving2002episodic,
  title={Episodic memory: From mind to brain},
  author={Tulving, Endel},
  journal={Annual review of psychology},
  volume={53},
  number={1},
  pages={1--25},
  year={2002},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@article{schacter2007remembering,
  title={Remembering the past to imagine the future: the prospective brain},
  author={Schacter, Daniel L and Addis, Donna Rose and Buckner, Randy L},
  journal={Nature reviews neuroscience},
  volume={8},
  number={9},
  pages={657--661},
  year={2007},
  publisher={Nature Publishing Group UK London}
}
@article{hassabis2007deconstructing,
  title={Deconstructing episodic memory with construction},
  author={Hassabis, Demis and Maguire, Eleanor A},
  journal={Trends in cognitive sciences},
  volume={11},
  number={7},
  pages={299--306},
  year={2007},
  publisher={Elsevier}
}



@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@inproceedings{zhang2024bench,
  title={∞ Bench: Extending Long Context Evaluation Beyond 100K Tokens},
  author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15262--15277},
  year={2024}
}

@article{squire1998episodic,
  title={Episodic memory, semantic memory, and amnesia},
  author={Squire, Larry R and Zola, Stuart M},
  journal={Hippocampus},
  volume={8},
  number={3},
  pages={205--211},
  year={1998},
  publisher={Wiley Online Library}
}

@article{mcclelland1995there,
  title={Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.},
  author={McClelland, James L and McNaughton, Bruce L and O'Reilly, Randall C},
  journal={Psychological review},
  volume={102},
  number={3},
  pages={419},
  year={1995},
  publisher={American Psychological Association}
}

@article{spreng2012remember,
  title={I remember you: a role for memory in social cognition and the functional neuroanatomy of their interaction},
  author={Spreng, R Nathan and Mar, Raymond A},
  journal={Brain research},
  volume={1428},
  pages={43--50},
  year={2012},
  publisher={Elsevier}
}

@article{gaesser2014episodic,
  title={Episodic simulation and episodic memory can increase intentions to help others},
  author={Gaesser, Brendan and Schacter, Daniel L},
  journal={Proceedings of the National Academy of Sciences},
  volume={111},
  number={12},
  pages={4415--4420},
  year={2014},
  publisher={National Academy of Sciences}
}

@article{moscovitch2016episodic,
  title={Episodic memory and beyond: the hippocampus and neocortex in transformation},
  author={Moscovitch, Morris and Cabeza, Roberto and Winocur, Gordon and Nadel, Lynn},
  journal={Annual review of psychology},
  volume={67},
  number={1},
  pages={105--134},
  year={2016},
  publisher={Annual Reviews}
}

@article{greenberg2010interdependence,
  title={Interdependence of episodic and semantic memory: Evidence from neuropsychology},
  author={Greenberg, Daniel L and Verfaellie, Mieke},
  journal={Journal of the International Neuropsychological society},
  volume={16},
  number={5},
  pages={748--753},
  year={2010},
  publisher={Cambridge University Press}
}

@article{george2009towards,
  title={Towards a mathematical theory of cortical micro-circuits},
  author={George, Dileep and Hawkins, Jeff},
  journal={PLoS computational biology},
  volume={5},
  number={10},
  pages={e1000532},
  year={2009},
  publisher={Public Library of Science San Francisco, USA}
}

@article{Botvinik2008,
author = {Botvinick, Matthew},
year = {2008},
month = {06},
pages = {201-8},
title = {Hierarchical models of behavior and prefrontal function},
volume = {12},
journal = {Trends in cognitive sciences},
doi = {10.1016/j.tics.2008.02.009}
}
@article{felleman1991distributed,
  title={Distributed hierarchical processing in the primate cerebral cortex.},
  author={Felleman, Daniel J and Van Essen, David C},
  journal={Cerebral cortex (New York, NY: 1991)},
  volume={1},
  number={1},
  pages={1--47},
  year={1991}
}

@article{eichenbaum2004hippocampus,
  title={Hippocampus: cognitive processes and neural representations that underlie declarative memory},
  author={Eichenbaum, Howard},
  journal={Neuron},
  volume={44},
  number={1},
  pages={109--120},
  year={2004},
  publisher={Elsevier}
}

@article{teyler1986hippocampal,
  title={The hippocampal memory indexing theory.},
  author={Teyler, Timothy J and DiScenna, Pascal},
  journal={Behavioral neuroscience},
  volume={100},
  number={2},
  pages={147},
  year={1986},
  publisher={American Psychological Association}
}
@article{rolls2013quantitative,
  title={A quantitative theory of the functions of the hippocampal CA3 network in memory},
  author={Rolls, Edmund T},
  journal={Frontiers in cellular neuroscience},
  volume={7},
  pages={98},
  year={2013},
  publisher={Frontiers Media SA}
}
@article{wilson1994reactivation,
  title={Reactivation of hippocampal ensemble memories during sleep},
  author={Wilson, Matthew A and McNaughton, Bruce L},
  journal={Science},
  volume={265},
  number={5172},
  pages={676--679},
  year={1994},
  publisher={American Association for the Advancement of Science}
}
@article{olafsdottir2018role,
  title={The role of hippocampal replay in memory and planning},
  author={{\'O}lafsd{\'o}ttir, H Freyja and Bush, Daniel and Barry, Caswell},
  journal={Current Biology},
  volume={28},
  number={1},
  pages={R37--R50},
  year={2018},
  publisher={Elsevier}
}
@article{louie2001temporally,
  title={Temporally structured replay of awake hippocampal ensemble activity during rapid eye movement sleep},
  author={Louie, Kenway and Wilson, Matthew A},
  journal={Neuron},
  volume={29},
  number={1},
  pages={145--156},
  year={2001},
  publisher={Elsevier}
}
@article{rasch2013sleep,
  title={About sleep's role in memory},
  author={Rasch, Bj{\"o}rn and Born, Jan},
  journal={Physiological reviews},
  year={2013},
  publisher={American Physiological Society Bethesda, MD}
}

@article{leng2024long,
  title={Long context rag performance of large language models},
  author={Leng, Quinn and Portes, Jacob and Havens, Sam and Zaharia, Matei and Carbin, Michael},
  journal={arXiv preprint arXiv:2411.03538},
  year={2024}
}

@article{wang2024multimodal,
  title={Multimodal needle in a haystack: Benchmarking long-context capability of multimodal large language models},
  author={Wang, Hengyi and Shi, Haizhou and Tan, Shiwei and Qin, Weiyi and Wang, Wenyuan and Zhang, Tunyu and Nambi, Akshay and Ganu, Tanuja and Wang, Hao},
  journal={arXiv preprint arXiv:2406.11230},
  year={2024}
}

@inproceedings{shi2023large,
  title={Large language models can be easily distracted by irrelevant context},
  author={Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\"a}rli, Nathanael and Zhou, Denny},
  booktitle={International Conference on Machine Learning},
  pages={31210--31227},
  year={2023},
  organization={PMLR}
}

@inproceedings{cormack2009reciprocal,
  title={Reciprocal rank fusion outperforms condorcet and individual rank learning methods},
  author={Cormack, Gordon V and Clarke, Charles LA and Buettcher, Stefan},
  booktitle={Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval},
  pages={758--759},
  year={2009}
}
@article{merola2025reconstructing,
  title={Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation},
  author={Merola, Carlo and Singh, Jaspinder},
  journal={arXiv preprint arXiv:2504.19754},
  year={2025}
}

@article{team2025gemma,
  title={Gemma 3 technical report},
  author={Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
  journal={arXiv preprint arXiv:2503.19786},
  year={2025}
}


@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{kovcisky2018narrativeqa,
  title={The narrativeqa reading comprehension challenge},
  author={Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={317--328},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@techreport{hong2025context,
  title = {Context Rot: How Increasing Input Tokens Impacts LLM Performance},
  author = {Hong, Kelly and Troynikov, Anton and Huber, Jeff},
  year = {2025},
  month = {July},
  institution = {Chroma},
  url = {https://research.trychroma.com/context-rot},
}

@inproceedings{xanh2020_2wikimultihop,
    title = "Constructing A Multi-hop {QA} Dataset for Comprehensive Evaluation of Reasoning Steps",
    author = "Ho, Xanh  and
      Duong Nguyen, Anh-Khoa  and
      Sugawara, Saku  and
      Aizawa, Akiko",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.580",
    pages = "6609--6625",
}

@misc{trivedi2022musiquemultihopquestionssinglehop,
      title={MuSiQue: Multihop Questions via Single-hop Question Composition}, 
      author={Harsh Trivedi and Niranjan Balasubramanian and Tushar Khot and Ashish Sabharwal},
      year={2022},
      eprint={2108.00573},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.00573}, 
}